{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhika3131/Deep_Learning_with_Python/blob/main/ch_04_Fundamentals_of_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaBUzbzUgeOy"
      },
      "source": [
        "# Generalization : The goal of machine Learning\n",
        "\n",
        "Te fundamental issue in machine learning is the tenaion between optimization and generalization . **Optimization** refers to the process of adjusting a model to get the best performance possible on training data(The learning in machine learning)\n",
        "\n",
        "**Generalization** refers to how well trained model performs on data it has never seen before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Rb2PRjziKLf"
      },
      "source": [
        "#Underfitting and Overfitting\n",
        "\n",
        "at the beginning of training , optimization and generalization are correlated , lower the loss on training data , the lower the loss on test data , while this is happening your model is **underfit**.\n",
        "\n",
        "**overfitting** - when we get better accuracy on training data but less on testing data.\n",
        "\n",
        "overfitting likely to occur when your data is noisy, if it involves uncertainity , or it includes rare features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeuDG0gnvAGW"
      },
      "source": [
        "**Adding white noise channels or all zeros channels to MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElyTev4agjyN",
        "outputId": "7850ffd8-c9b2-4d74-cba2-c63bbfdc29f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(train_images, train_labels), _ = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "train_images_with_noise_channels = np.concatenate(\n",
        "    [train_images, np.random.random((len(train_images), 784))], axis=1)\n",
        "\n",
        "train_images_with_zeros_channels = np.concatenate(\n",
        "    [train_images, np.zeros((len(train_images), 784))], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL4_Zk5bvnfc"
      },
      "source": [
        "**Training the same model on MNIST data with noise channels or all-zeros channels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baa77L1DvmKk",
        "outputId": "93392102-c6ef-4891-9538-073c915318c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 6s 14ms/step - loss: 0.6182 - accuracy: 0.8108 - val_loss: 0.3038 - val_accuracy: 0.9059\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.2552 - accuracy: 0.9218 - val_loss: 0.2101 - val_accuracy: 0.9358\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.1670 - accuracy: 0.9483 - val_loss: 0.1617 - val_accuracy: 0.9526\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.1184 - accuracy: 0.9624 - val_loss: 0.1502 - val_accuracy: 0.9552\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0857 - accuracy: 0.9729 - val_loss: 0.1240 - val_accuracy: 0.9646\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.0631 - accuracy: 0.9802 - val_loss: 0.1216 - val_accuracy: 0.9668\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0448 - accuracy: 0.9859 - val_loss: 0.1274 - val_accuracy: 0.9643\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 6s 16ms/step - loss: 0.0327 - accuracy: 0.9895 - val_loss: 0.1203 - val_accuracy: 0.9670\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0251 - accuracy: 0.9919 - val_loss: 0.1240 - val_accuracy: 0.9684\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 9s 24ms/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.1299 - val_accuracy: 0.9680\n",
            "Epoch 1/10\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.2969 - accuracy: 0.9153 - val_loss: 0.1550 - val_accuracy: 0.9552\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.1230 - accuracy: 0.9629 - val_loss: 0.1106 - val_accuracy: 0.9663\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0816 - accuracy: 0.9765 - val_loss: 0.0899 - val_accuracy: 0.9724\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.0589 - accuracy: 0.9824 - val_loss: 0.0801 - val_accuracy: 0.9757\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0440 - accuracy: 0.9878 - val_loss: 0.0821 - val_accuracy: 0.9754\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.0337 - accuracy: 0.9902 - val_loss: 0.0757 - val_accuracy: 0.9793\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 0.0745 - val_accuracy: 0.9787\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 5s 13ms/step - loss: 0.0187 - accuracy: 0.9951 - val_loss: 0.0844 - val_accuracy: 0.9768\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 6s 15ms/step - loss: 0.0143 - accuracy: 0.9964 - val_loss: 0.0802 - val_accuracy: 0.9779\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 5s 14ms/step - loss: 0.0104 - accuracy: 0.9977 - val_loss: 0.0734 - val_accuracy: 0.9808\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(512, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "history_noise = model.fit(\n",
        "    train_images_with_noise_channels, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2)\n",
        "\n",
        "model = get_model()\n",
        "history_zeros = model.fit(\n",
        "    train_images_with_zeros_channels, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXdyg3iAwJM4"
      },
      "source": [
        "**Plotting a validation accuracy comparision**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "M63HqMl0v-kn",
        "outputId": "e763eeaf-8a09-4602-9718-c4871a94ac1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd28fb91ee0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_acc_noise = history_noise.history[\"val_accuracy\"]\n",
        "val_acc_zeros = history_zeros.history[\"val_accuracy\"]\n",
        "epochs = range(1, 11)\n",
        "plt.plot(epochs, val_acc_noise, \"b-\",\n",
        "         label=\"Validation accuracy with noise channels\")\n",
        "plt.plot(epochs, val_acc_zeros, \"b--\",\n",
        "         label=\"Validation accuracy with zeros channels\")\n",
        "plt.title(\"Effect of noise channels on validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGxPOlJOxU3Y"
      },
      "source": [
        "Despite the data holding the same information in both cases , the validation accuracy of the model trained with noise channels ends up about one percentage point lower , purely through the influence of spurious correlation . The more noise channels you add , the further accuracy will degrade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbK3l2PVyDkg"
      },
      "source": [
        "#The nature of generalization in Deep Learning\n",
        "\n",
        "The remarkable fact about deep learning model is that they can be trained to fit anything , as long as they enough representational power \n",
        "\n",
        "Try shuffling the MNISt labels and train a model on that.Even though there is no relationship whatsoever between the inputs and the shuffled labels , the training loss goes down just fine , even with a relatively small model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNA4XX5XzXt_"
      },
      "source": [
        "**Fitting a MNIST model with randomly shuffled labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOVY1bQCx4_O",
        "outputId": "7fee765a-e592-4c08-fa98-50552789da3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "375/375 [==============================] - 4s 9ms/step - loss: 2.3148 - accuracy: 0.1033 - val_loss: 2.3048 - val_accuracy: 0.1083\n",
            "Epoch 2/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 2.2991 - accuracy: 0.1167 - val_loss: 2.3129 - val_accuracy: 0.0984\n",
            "Epoch 3/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 2.2906 - accuracy: 0.1268 - val_loss: 2.3152 - val_accuracy: 0.1055\n",
            "Epoch 4/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 2.2794 - accuracy: 0.1375 - val_loss: 2.3255 - val_accuracy: 0.0997\n",
            "Epoch 5/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 2.2633 - accuracy: 0.1494 - val_loss: 2.3408 - val_accuracy: 0.0998\n",
            "Epoch 6/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 2.2435 - accuracy: 0.1637 - val_loss: 2.3492 - val_accuracy: 0.1015\n",
            "Epoch 7/100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 2.2215 - accuracy: 0.1814 - val_loss: 2.3612 - val_accuracy: 0.1013\n",
            "Epoch 8/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 2.1957 - accuracy: 0.1953 - val_loss: 2.3821 - val_accuracy: 0.0970\n",
            "Epoch 9/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 2.1676 - accuracy: 0.2114 - val_loss: 2.3890 - val_accuracy: 0.1028\n",
            "Epoch 10/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 2.1355 - accuracy: 0.2280 - val_loss: 2.4289 - val_accuracy: 0.1038\n",
            "Epoch 11/100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 2.1024 - accuracy: 0.2415 - val_loss: 2.4523 - val_accuracy: 0.0998\n",
            "Epoch 12/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 2.0647 - accuracy: 0.2596 - val_loss: 2.4766 - val_accuracy: 0.0994\n",
            "Epoch 13/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 2.0293 - accuracy: 0.2751 - val_loss: 2.4902 - val_accuracy: 0.1016\n",
            "Epoch 14/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 1.9911 - accuracy: 0.2919 - val_loss: 2.5251 - val_accuracy: 0.1013\n",
            "Epoch 15/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 1.9520 - accuracy: 0.3079 - val_loss: 2.5535 - val_accuracy: 0.1019\n",
            "Epoch 16/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.9122 - accuracy: 0.3259 - val_loss: 2.6033 - val_accuracy: 0.1013\n",
            "Epoch 17/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.8736 - accuracy: 0.3438 - val_loss: 2.6457 - val_accuracy: 0.1021\n",
            "Epoch 18/100\n",
            "375/375 [==============================] - 4s 9ms/step - loss: 1.8325 - accuracy: 0.3595 - val_loss: 2.6737 - val_accuracy: 0.1000\n",
            "Epoch 19/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.7947 - accuracy: 0.3752 - val_loss: 2.7004 - val_accuracy: 0.1019\n",
            "Epoch 20/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.7550 - accuracy: 0.3915 - val_loss: 2.7567 - val_accuracy: 0.1012\n",
            "Epoch 21/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.7179 - accuracy: 0.4055 - val_loss: 2.7924 - val_accuracy: 0.1009\n",
            "Epoch 22/100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 1.6812 - accuracy: 0.4173 - val_loss: 2.8529 - val_accuracy: 0.1020\n",
            "Epoch 23/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.6434 - accuracy: 0.4358 - val_loss: 2.8808 - val_accuracy: 0.0980\n",
            "Epoch 24/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.6067 - accuracy: 0.4479 - val_loss: 2.9352 - val_accuracy: 0.1024\n",
            "Epoch 25/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.5737 - accuracy: 0.4596 - val_loss: 2.9456 - val_accuracy: 0.1011\n",
            "Epoch 26/100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 1.5353 - accuracy: 0.4751 - val_loss: 2.9986 - val_accuracy: 0.1011\n",
            "Epoch 27/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.5023 - accuracy: 0.4875 - val_loss: 3.0450 - val_accuracy: 0.0993\n",
            "Epoch 28/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.4715 - accuracy: 0.4960 - val_loss: 3.0941 - val_accuracy: 0.1001\n",
            "Epoch 29/100\n",
            "375/375 [==============================] - 4s 12ms/step - loss: 1.4383 - accuracy: 0.5120 - val_loss: 3.1582 - val_accuracy: 0.1058\n",
            "Epoch 30/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.4063 - accuracy: 0.5202 - val_loss: 3.2205 - val_accuracy: 0.1042\n",
            "Epoch 31/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.3775 - accuracy: 0.5346 - val_loss: 3.2544 - val_accuracy: 0.1032\n",
            "Epoch 32/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.3444 - accuracy: 0.5449 - val_loss: 3.3019 - val_accuracy: 0.0995\n",
            "Epoch 33/100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 1.3190 - accuracy: 0.5510 - val_loss: 3.3605 - val_accuracy: 0.1025\n",
            "Epoch 34/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.2903 - accuracy: 0.5644 - val_loss: 3.4057 - val_accuracy: 0.1010\n",
            "Epoch 35/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.2614 - accuracy: 0.5752 - val_loss: 3.4821 - val_accuracy: 0.0998\n",
            "Epoch 36/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 1.2304 - accuracy: 0.5865 - val_loss: 3.5168 - val_accuracy: 0.1021\n",
            "Epoch 37/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 1.2088 - accuracy: 0.5927 - val_loss: 3.5805 - val_accuracy: 0.0983\n",
            "Epoch 38/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.1815 - accuracy: 0.6041 - val_loss: 3.6311 - val_accuracy: 0.1006\n",
            "Epoch 39/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.1538 - accuracy: 0.6136 - val_loss: 3.7150 - val_accuracy: 0.1041\n",
            "Epoch 40/100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 1.1337 - accuracy: 0.6230 - val_loss: 3.7804 - val_accuracy: 0.0988\n",
            "Epoch 41/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.1095 - accuracy: 0.6304 - val_loss: 3.7951 - val_accuracy: 0.1017\n",
            "Epoch 42/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.0837 - accuracy: 0.6400 - val_loss: 3.8999 - val_accuracy: 0.0993\n",
            "Epoch 43/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.0611 - accuracy: 0.6469 - val_loss: 3.9486 - val_accuracy: 0.1030\n",
            "Epoch 44/100\n",
            "375/375 [==============================] - 4s 11ms/step - loss: 1.0431 - accuracy: 0.6521 - val_loss: 4.0058 - val_accuracy: 0.1006\n",
            "Epoch 45/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 1.0170 - accuracy: 0.6645 - val_loss: 4.0670 - val_accuracy: 0.1023\n",
            "Epoch 46/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.9988 - accuracy: 0.6699 - val_loss: 4.1256 - val_accuracy: 0.1028\n",
            "Epoch 47/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.9772 - accuracy: 0.6773 - val_loss: 4.1968 - val_accuracy: 0.0982\n",
            "Epoch 48/100\n",
            "375/375 [==============================] - 4s 9ms/step - loss: 0.9592 - accuracy: 0.6844 - val_loss: 4.2144 - val_accuracy: 0.1042\n",
            "Epoch 49/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.9367 - accuracy: 0.6911 - val_loss: 4.3288 - val_accuracy: 0.1024\n",
            "Epoch 50/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.9205 - accuracy: 0.6980 - val_loss: 4.3595 - val_accuracy: 0.1027\n",
            "Epoch 51/100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.9000 - accuracy: 0.7045 - val_loss: 4.4511 - val_accuracy: 0.0994\n",
            "Epoch 52/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.8825 - accuracy: 0.7097 - val_loss: 4.4763 - val_accuracy: 0.1051\n",
            "Epoch 53/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.8632 - accuracy: 0.7186 - val_loss: 4.5672 - val_accuracy: 0.1039\n",
            "Epoch 54/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.8460 - accuracy: 0.7224 - val_loss: 4.6337 - val_accuracy: 0.1014\n",
            "Epoch 55/100\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 0.8289 - accuracy: 0.7283 - val_loss: 4.7365 - val_accuracy: 0.0994\n",
            "Epoch 56/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.8126 - accuracy: 0.7341 - val_loss: 4.7787 - val_accuracy: 0.0980\n",
            "Epoch 57/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.7944 - accuracy: 0.7415 - val_loss: 4.8209 - val_accuracy: 0.1005\n",
            "Epoch 58/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.7808 - accuracy: 0.7460 - val_loss: 4.8964 - val_accuracy: 0.1018\n",
            "Epoch 59/100\n",
            "375/375 [==============================] - 3s 9ms/step - loss: 0.7653 - accuracy: 0.7506 - val_loss: 4.9592 - val_accuracy: 0.1005\n",
            "Epoch 60/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.7473 - accuracy: 0.7573 - val_loss: 5.0289 - val_accuracy: 0.1039\n",
            "Epoch 61/100\n",
            "150/375 [===========>..................] - ETA: 1s - loss: 0.7025 - accuracy: 0.7773"
          ]
        }
      ],
      "source": [
        "(train_images, train_labels), _ = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "random_train_labels = train_labels[:]\n",
        "np.random.shuffle(random_train_labels)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, random_train_labels,\n",
        "          epochs=100,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The manifold hypothesis**\n",
        "\n",
        "**Interpolation as a source of generalization**\n",
        "\n",
        "**Why deep learning works**\n",
        "\n",
        "A deep learning model is basically a very high-dimensional curve-a curve that is smooth and continuous (with additional constraints on its structure, originating from model architecture priors), since it needs to be differentiable. And that curve is fitted to data points via gradient descent, smoothly and incrementally. By its very nature, deep learning is about taking a big, complex curve-a manifold-and incrementally adjusting its parameters until it fits some training data points.\n",
        "\n",
        "\n",
        "The curve involves enough parameters that it could fit anything—indeed, if you let your model train for long enough, it will effectively end up purely memorizing its training data and won't generalize at all. However, the data you're fitting to isn't made of isolated points sparsely distributed across the underlying space. Your data forms a highly structured, low-dimensional manifold within the input space-that's the mani- fold hypothesis. And because fitting your model curve to this data happens gradually and smoothly over time as gradient descent progresses, there will be an intermediate point during training at which the model roughly approximates the natural manifold of the data,\n",
        "\n",
        "**Training data is paramount**\n",
        "\n",
        "#Evaluating machine-learning models\n",
        "\n",
        "**Training, validation, and test sets**\n",
        "\n",
        "Evaluating a model always boils down to splitting the available data into three sets: training, validation, and test. You train on the training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data, which is meant to be as similar as possible to production data. Then you can deploy the model in production.\n",
        "\n",
        "You y ask, why not have two sets: a training set and a test set? You'd train on the may training data and evaluate on the test data. Much simpler!\n",
        "\n",
        "The reason is that developing a model always involves tuning its configuration: for example, choosing the number of layers or the size of the layers (called the hyperpa- rameters of the model, to distinguish them from the parameters, which are the network's weights). You do this tuning by using as a feedback signal the performance of the model on the validation data. In essence, this tuning is a form of learning a search for a good configuration in some parameter space. As a result, tuning the configuration of the model based on its performance on the validation set can quickly result in over-\n",
        "\n",
        "fitting to the validation set, even though your model is never directly trained on it\n",
        "\n",
        "Central to this phenomenon is the notion of information leaks. Every time you tune a hyperparameter of your model based on the model's performance on the validation set, some information about the validation data leaks into the model. If you do this only once, for one parameter, then very few bits of information will leak, and your val idation set will remain reliable for evaluating the model. But if you repeat this many times-running one experiment, evaluating on the validation set, and modifying your model as a result-then you'll leak an increasingly significant amount of information about the cation set into the model.\n",
        "\n",
        "At the end the day, you'll end up with a model that performs artificially well on the wild because that's what you optimized it for. You care about perfor mance on couge new data, not on the validation data, so you need to use a com pletely different, never-before-seen dataset to evaluate the model: the test dataset Your model shouldn't have had access to any information about the test set, even indi- rectly. If anything about the model has been tuned based on test set performance, then your measure of generalization will be flawed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Splitting your data into training, validation, and test sets may seem but there are a few advanced ways to do it that can come in handy when little data is available. Let's review three classic evaluation :\n",
        "\n",
        "**Simple hold-out validation**\n",
        "\n",
        "Set apart some fraction of your data as your test set. Train on the remaining data, and evaluate on the test set. As you saw in the previous sections, in order to prevent infor mation leaks, you shouldn't tune your model based on the test set, and therefore you should also reserve a validation set.\n",
        "\n",
        "This is the simplest evaluation protocol, and it suffers from one flaw: if little data is available, then your validation and test sets may contain too few samples to be statisti- cally representative of the data at hand. This is easy to recognize: if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance, then you're having this issue. K-fold validation and iterated K-fold validation are two ways to address this, as discussed next.\n",
        "\n",
        "\n",
        "**K-fold validation**\n",
        "\n",
        "With this approach, you split your data into K partitions of equal size. For each parti- tion i, train a model on the remaining K 1 partitions, and evaluate it on partition i. - Your final score is then the averages of the K scores obtained. This method is helpful when the performance of your model shows significant variance based on your train- test split. Like holdout validation, this method doesn't exempt you from using a dis- tinct validation set for model calibration.\n",
        "\n",
        "\n",
        "**Iterated K-fold validation with shuffling**\n",
        "\n",
        "This one is for situations in which you have relatively little data available and vou need to evaluate your model as precisely as possible. I've found it to be extremely elpful in Kaggle competitions. It consists of applying K-fold validation multiple times, huffling the data every time before splitting it K ways. The final score is the avera of the scores obtained at each run of K-fold validation. Note that you end up trai ng and evaluating PK models (where P is the number of iterations you use), which can be very expensive.\n",
        "**Beating a common-sense baseline**\n",
        "\n",
        "**Things to keep in mind about model evaluation**\n",
        "\n",
        "#Improving model fit\n",
        "\n",
        "To achieve the perfect fit, you must first overfit. Since you don't know in advance where the boundary lies, you must cross it to find it. Thus, your initial goal as you start working on a problem is to achieve a model that shows some generalization power and that is able to overfit. Once you have such a model, you'll focus on refining gener- alization by fighting overfitting.\n",
        "\n",
        "There are three common problems you'll encounter at this stage:\n",
        "\n",
        "1. Training doesn't get started: your training loss doesn't go down over time.  2. Training gets started just fine, but your model doesn't meaningfully generalize: you can't beat the common-sense baseline you set.\n",
        "3. Training and validation loss both go down over time, and you can beat your baseline, but you don't seem to be able to overfit, which indicates you're still underfitting.\n",
        "\n",
        "**Tuning key gradient descent parameters**\n",
        "\n",
        "Sometimes training doesn't get started, or it stalls too early. Your loss is stuck. This is always something you can overcome: remember that you can fit a model to random data\n",
        "When this happens, it's always a problem with the configuration of the gradient descent process: your choice of optimizer, the distribution of initial values in the weights of your model, your learning rate, or your batch size.\n",
        "\n",
        "**Training a MNIST model with an incorrectly high learning rate**\n"
      ],
      "metadata": {
        "id": "SrdcbdY5BH3b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sg6NISazskq"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), _ = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(1.),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model quickly reaches a training and validation accuracy in the 30%-40% range but cannot get past that. Let's try to lower the learning rate to a more reasonable value of 1e-2."
      ],
      "metadata": {
        "id": "V16RPIEWJCzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The same model with a more appropriate learning rate**"
      ],
      "metadata": {
        "id": "1bvPxbvBCVKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(1e-2),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "id": "gDj8SkBGCbS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is now able to train.\n",
        "\n",
        "If you find yourself in a similar situation, try\n",
        "\n",
        "■Lowering or increasing the learning rate. A learning rate that is too high may lead to updates that vastly overshoot a proper fit, like in the preceding example, and a learning rate that is too low may make training so slow that it appears to stall.\n",
        "\n",
        "• Increasing the batch size. A batch with more samples will lead to gradients that\n",
        "\n",
        "are more informative and less noisy (lower variance).\n"
      ],
      "metadata": {
        "id": "hN5VqTDAJmzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Leveraging better architecture priors\n",
        "\n",
        "You have a model that fits  but for some reason your validation metrics aren't improv ing at all. They remain no better than what a random classifier would achieve: your model train but doesn't generalize. What's going on?\n",
        "\n",
        "This is perhaps the worst machine learning situation you can find yourself in. It indicates that something is fundamentally wrong with your approach, and it may not be easy to tell what. Here are some tips.\n",
        "\n",
        "First, it may be that the input data you're using simply doesn't contain sufficient information to predict your targets\n",
        "\n",
        "**Increasing model capacity**\n",
        "\n",
        "**A simple logistic regression on MNIST**\n",
        "\n",
        "If you manage to get to a model that fits, where validation metrics are going down, and that seems to achieve at least some level of generalization power, congratulations you're almost there. Next, you need to get your model to start overfitting."
      ],
      "metadata": {
        "id": "o0ocEvFzChPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_small_model = model.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2)"
      ],
      "metadata": {
        "id": "nNk6NcLKCpha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_loss = history_small_model.history[\"val_loss\"]\n",
        "epochs = range(1, 21)\n",
        "plt.plot(epochs, val_loss, \"b--\",\n",
        "         label=\"Validation loss\")\n",
        "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "k1hlpShLCxts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation metrics seem to stall, or to improve very slowly, instead of peaking and reversing course. The validation loss goes to 0.26 and just stays there. You can fit, but you can't clearly overfit, even after many iterations over the training data. You're likely to encounter similar curves often in your career.\n",
        "\n",
        "Remember that it should always be possible to overfit. Much like the problem where the training loss doesn't go down, this is an issue that can always be solved. "
      ],
      "metadata": {
        "id": "sqHEQA0acnzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you can't seem to be able to overfit, it's likely a problem with the representational power of your model: you're going to need a bigger model, one with more capacity, that is to say, one able to store more information. You can increase representational power by adding more layers, using bigger layers (layers with more parameters), or using kinds of layers that are more appropriate for the problem at hand (better architecture priors).\n",
        "\n",
        "Lets try traning a bigger model "
      ],
      "metadata": {
        "id": "VtNAgOTbdGmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(96, activation=\"relu\"),\n",
        "    layers.Dense(96, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_large_model = model.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2)"
      ],
      "metadata": {
        "id": "uutjNaxzCyZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_loss = history_large_model.history[\"val_loss\"]\n",
        "epochs = range(1, 21)\n",
        "plt.plot(epochs, val_loss, \"b--\",\n",
        "         label=\"Validation loss\")\n",
        "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "YGpIHRATdgWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Improving generalization\n",
        "\n",
        "**Dataset curation**\n",
        "As such, it is essential that you make sure that you're working with an appropriate dataset. Spending more effort and money on data collection almost always yields a much greater return on investment than spending the same on developing a better model.\n",
        "\n",
        "* Make sure you have enough data. Remember that you need a dense sampling of the input-cross-output space. More data will yield a better model. Sometimes problems that seem impossible at first become solvable with a larger dataset. ■ Minimize labeling errors-visualize your inputs to check for anomalies, and proofread your labels.\n",
        "\n",
        "■ Clean your data and deal with missing values (we'll cover this in the next chapter). If you have many features and you aren't sure which ones are actually useful, do\n",
        "\n",
        "A particularly important way to improve the generalization potential of your data is feature engineering.\n",
        "\n",
        "**Feature engineering**\n",
        "Feature engineering is the process of using your own knowledge about the data and about the machine learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (non-learned) transformations to the data before it goes into the model. In many cases, it isn't reasonable to expect a machine learning model to be able to learn from completely arbitrary data. The data needs to be presented to the model in a way that will make the model's job easier.\n",
        "\n",
        "Good features still allow you to solve problems more elegantly while using fewer resources. For instance, it would be ridiculous to solve the problem of reading a clock face using a convolutional neural network.\n",
        "\n",
        "Good features let you solve a problem with far less data. The ability of deep learning models to learn features on their own relies on having lots of training data available; if you have only a few samples, the information value in their fea tures becomes critical.\n",
        "\n",
        "**Using early stopping**\n",
        "\n",
        "Finding the exact point during training where you've reached the most generaliz able fit-the exact boundary between an underfit curve and an overfit curve-is one of the most effective things you can do to improve generalization.\n",
        "\n",
        "In the examples in the previous chapter, we would start by training our models for longer than needed to figure out the number of epochs that yielded the best valida tion metrics, and then we would retrain a new model for exactly that number of epochs. This is pretty standard, but it requires you to do redundant work, which can sometimes be expensive. Naturally, you could just save your model at the end of each epoch, and once you've found the best epoch, reuse the closest saved model you have. In Keras, it's typical to do this with an EarlyStopping callback, which will interrupt training as soon as validation metrics have stopped improving, while best known model state\n",
        "\n",
        "#Regularizing your model\n",
        "Regularization techniques are a set of best practices that actively impede the model's abil- ity to fit perfectly to the training data, with the goal of making the model perform bet- ter during validation. This is called \"regularizing\" the model, because it tends to make the model simpler, more \"regular,\" its curve smoother, more \"generic\"; thus it is less specific to the training set and better able to generalize by more closely approximat- ing the latent manifold of the data.\n",
        "\n",
        "**Regularization techniques**:\n",
        "\n",
        "**Reducing the network's size**\n",
        "\n",
        "Original model"
      ],
      "metadata": {
        "id": "cxRBbOiVC3hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "train_data = vectorize_sequences(train_data)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_original = model.fit(train_data, train_labels,\n",
        "                             epochs=20, batch_size=512, validation_split=0.4)"
      ],
      "metadata": {
        "id": "M6nnfzQVDZhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version of the model with lower capacity**"
      ],
      "metadata": {
        "id": "aJawDHWKDaMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(4, activation=\"relu\"),\n",
        "    layers.Dense(4, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_smaller_model = model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=20, batch_size=512, validation_split=0.4)"
      ],
      "metadata": {
        "id": "hnqOecygDfDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding weight regularization\n",
        "\n",
        "**Adding L2 weight regularization to the model**"
      ],
      "metadata": {
        "id": "RsNyS-98D_oK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16,\n",
        "                 kernel_regularizer=regularizers.l2(0.002),\n",
        "                 activation=\"relu\"),\n",
        "    layers.Dense(16,\n",
        "                 kernel_regularizer=regularizers.l2(0.002),\n",
        "                 activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_l2_reg = model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=20, batch_size=512, validation_split=0.4)"
      ],
      "metadata": {
        "id": "Q68pFOgTDi02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Different weight regularizers available in Keras**"
      ],
      "metadata": {
        "id": "q_DAiF0_D2_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "regularizers.l1(0.001)\n",
        "regularizers.l1_l2(l1=0.001, l2=0.001)"
      ],
      "metadata": {
        "id": "qY6-8aEtDmQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding dropout\n",
        "\n",
        "**Adding dropout to the IMDB model**"
      ],
      "metadata": {
        "id": "NNgewO8ZDumv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_dropout = model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=20, batch_size=512, validation_split=0.4)"
      ],
      "metadata": {
        "id": "uJTI7UKGDpt4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFrC6ggVL0roUhtGmKyqZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}