{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+Hp6OOwhC96JHQ0H4xODw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhika3131/Deep_Learning_with_Python/blob/main/ch_05_The_Universal_workflow_of_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The universal workflow of machine learning is broadly structured in three parts**:\n",
        "\n",
        "**Define the task**-Understand the problem domain and the business logic under- lying what the customer asked for. Collect a dataset, understand what the data represents, and choose how you will measure success on the task.\n",
        "\n",
        "**Develop a model**-Prepare your data so that it can be processed by a machine learning model, select a model evaluation protocol and a simple baseline to beat, train a first model that has generalization power and that can overfit, and then regularize and tune your model until you achieve the best possible gener alization performance,\n",
        "\n",
        "**Deploy the model**-Present your work to stakeholders, ship the model to a web server, a mobile app, a web page, or an embedded device, monitor the model's\n",
        "performance in the wild , and start collecting the data you will neend to build the next generation model"
      ],
      "metadata": {
        "id": "0Asawk9CEZ99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the task\n",
        "\n",
        "You can't do good work without a deep understanding of the context of what you're doing. Why is your customer trying to solve this particular problem? What value will they derive from the solution-how will your model be used, and how will it fit into your customer's business processes? What kind of data is available, or could be col- lected? What kind of machine learning task can be mapped to the business problem\n",
        "\n",
        "**Frame the problem**\n",
        "\n",
        "Framing a machine learning problem usually involves many detailed discussions with stakeholders. Here are the questions that should be on the top of y your mind:\n",
        "\n",
        "What will your input data be? What are you trying to predict? You can only learn to predict something if you have training data available: for example, you can only learn to classify the sentiment of movie reviews if you have both movie reviews and sentiment annotations available. As such, data availability is usually the limiting factor at this stage. In many cases, you will have to resort to collecting and annotating new datasets yourself (which we'll cover\n",
        "\n",
        "in the next section). What type of machine learning task are you facing? Is it binary classification? Multiclass classification? Scalar regression? Vector regression? Multiclass, multi- label classification? Image segmentation? Ranking? Something else, like cluster- ing, generation, or reinforcement learning? In some cases, it may be that machine learning isn't even the best way to make sense of the data, and you should use something else, such as plain old-school statistical analysis.\n",
        "\n",
        "The photo search engine project is a multiclass, multilabel classification task. detection project is a binary classification task. If you set \"offensive\n",
        "\n",
        "The\n",
        "\n",
        "spam content\" as a separate class, it's a three-way classification task. - The music recommendation engine turns out to be better handled not via deep learning, but via matrix factorization (collaborative filtering).\n",
        "\n",
        "The credit card fraud detection project is a binary classification task.\n",
        "\n",
        "Once you've done your research, you should know what your inputs will be, what your targets will be, and what broad type of machine learning task the problem maps to aware of the hypotheses you're making at this stage:\n",
        "\n",
        "You hypothesize that your targets can be predicted given your inputs. You hypothesize that the data that's available (or that you will soon collect) is\n",
        "sufficiently informative to learn the relationship between inputs and targets."
      ],
      "metadata": {
        "id": "G0tMzUCXGbp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collect a dataset**\n",
        "\n",
        "Once you understand the nature of the task and you gets are going to be, it's time for data collection-the most arduous, time and costly part of most machine learning projects.\n",
        "\n",
        "**BEWARE OF NON-REPRESENTATIVE DATA**\n",
        "\n",
        "Machine learning models can only make sense of inputs that are similar to what your they've seen before. As such, it's critical that the data used for training should be repre sentative of the production data. This concern should be the foundation of all data collection work.\n",
        "\n",
        "\n",
        "\n",
        "Suppose you're developing an app where users can take pictures of a plate of food to find out the name of the dish. You train a model using pictures from an image-sharing social network that's popular with foodies. Come deployment time, feedback from angry users starts rolling in: your app gets the answer wrong 8 times out of 10. What's going on? Your accuracy on the test set was well over 90%! A quick look at user-uploaded data reveals that mobile picture uploads of random dishes from random restaurants taken with random smartphones look nothing like the professional-quality, well-i appetizing pictures you trained the model on: your training data wasn't representative of the production data. That's a cardinal sin-welcome to machine learning hell.\n",
        "\n",
        "If possible, collect data directly from the environment where your model will be used"
      ],
      "metadata": {
        "id": "wQyTnZISHu8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understand the problem**\n",
        "\n",
        "\n",
        "its pretty bad practice to treat a dataset as a black box. Before you start training mo els, you should explore and visualize your data to gain insights about what makes predictive, which will inform feature engineering and screen for potential issues\n",
        "\n",
        "If your data includes images or natural language text take a look at a few\n",
        "\n",
        "ples (and their labels) directly\n",
        "\n",
        "If your data contains numerical features, it's a good idea to plot the histogram of feature values to get a feel for the range of values taken and the frequency\n",
        "\n",
        "If your data includes location information, plot it on a map. Do any clear pas\n",
        "\n",
        "Are some samples missing values for some features? If so, you'll need to deal with this when you prepare the data (we'll cover how to do this in the nex\n",
        "\n",
        "If your task is a classification problem, print the number of instances of each class in your data. Are the classes roughly equally represented? If not, you will need to account for this imbalance\n",
        "\n",
        "heck for target leaking the presence of features in your data that provide infor mation about the targets and which may not be available in production. I you're training a model on medical records to predict whether someone will be treated for cancer in the future, and the records include the feature \"this per son has been diagnosed with cancer,\" then your targets are being artificially leaked into your data. Always ask yourself, is every feature in your data some thing that will be available in the same form in production?\n",
        "\n",
        "**Choose the measure of success**\n",
        "\n",
        "To control something, you need to be able to observe it. To achieve success on a proj ect, you must first define what you mean by success. Accuracy? Precision and recall: Customer retention rate? Your metric for success will guide all of the technical choices you make throughout the project. It should directly align with your higher-level goals such as the business success of your customer."
      ],
      "metadata": {
        "id": "NAu5mYqVI9Jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Develop the model\n",
        "\n",
        "**Prepare the data**\n",
        "you've learned before, deep learning models typically don't ingest raw data. Data preprocessing aims at making the raw data at hand more amenable to neural net- works. This includes vectorization, normalization, or handling missing values\n",
        "\n",
        "**VECTORIZATION**\n",
        "\n",
        "All inputs and targets in a neural network must typically be tensors of floating point data (or, in specific cases, tensors of integers or strings) Whatever data you need to process-sound, images, text-you must first turn into tensora step called data w ration For instance, in the two previous text-classification examples in chapter 4, we started with text represented as lists of integers (standing for sequences of words), and we used one-hot encoding to turn them into a tensor of float32 data. In the examples of classifying digits and predicting house prices, the data came in vectorized form, so we were able to skip this step.\n",
        "\n",
        "**NORMALISATION**\n",
        "\n",
        "In the MNIST digit-classification example from chapter 2, we started with image data encoded as integers in the 0-255 range, encoding grayscale values. Before we fed this data into our network, we had to cast it to float32 and divide by 255 so we'd end up\n",
        "\n",
        "with floating-point values in the 0-1 range. Similarly, when predicting house prices, we started with features that took a variety of ranges-some features had small floating- point values, and others had fairly large integer values. Before we fed this data into our network, we had to normalize each feature independently so that it had a stan- dard deviation of 1 and a mean of 0.\n",
        "\n",
        "In general, it isn't safe to feed into a neural network data that takes relatively large values (for example, multi-digit integers, which are much larger than the ini- fal values taken by the weights of a network) or data that is heterogeneous (for example, data where one feature is in the range 0-1 and another is in the range 100-200). Doing so can trigger large gradient updates that will prevent the network from converging. To make learning easier for your network, your data should have the following characteristics:\n",
        "\n",
        "Take small values-Typically, most values should be in the 0-1 range. \n",
        "\n",
        "Be homogenous-All features should take values in roughly the same range"
      ],
      "metadata": {
        "id": "ncbhiyBtQJNU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CePcz-6iRAJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}