{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzOpGPEVI9TAZO7hqAEonV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhika3131/Deep_Learning_with_Python/blob/main/ch_03_Getting_started_with_neural_network_Classification_and_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification and Regression glossary\n",
        "\n",
        "classification and regression involves mnay specialized terms .\n",
        "\n",
        "1. Sample or input- one data point that goes into your model\n",
        "2. Prediction or output - what comes out of you model .\n",
        "3. Target - The truth. what your model should ideally have predicted , according to an external source of data.\n",
        "4. classes - A set of possible labels to chose from in a classification problem .For example , when  classifying cat and dog pictures , \"dog\" and \"cat\" are the two classes.\n",
        "5. Label - A specific instance of class annotation in aclassification problem . For instance , if picture # 1234 is annotated as containing the class \"dog\" the \"dog\" is label of picture #1234\n",
        "6. Griund-Truth or annotations - All targets for a dataset, typically collected by humans.\n",
        "7. Binary Classification - A classification task where each input sample should be categorized into two exclusive categories.\n",
        "8. Multilabel Classification - A classification task where each input samples can be asssigned multiple labels , For instance , a given image may contain both a cat and dog and should be annnotated both with the \"cat\" label and the \"dog\" label .**The number of labels per image is usually varible**.\n",
        "9. Multiclass classification - A classification task where each input sample should be categorized into more than two categories:for instance , classifying handwritten digit.\n",
        "10. Scalar Regression - A task where the target is a  continuous scalar value .Predicting house price is agood example: the different target price from continuous space.\n",
        "11. Vector regression - a task where the target is a set of continuous values .for example , a continuous vector. if you are doing regression against  multiple values(such as coordinates of bounding box in a images.\n",
        "12. Mini- batch or Batch - a small set of samples(typically betwenn 8 and 128)\n",
        "that are processed simultaneously by the model . The number of samples is often a power of 2 , to facilitate the memory allocation of GPU . when training a mini-batch is used to ompute a single gradient descent update applied to the weights of the model\n"
      ],
      "metadata": {
        "id": "mhJDDLVqBgRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifying movie reviews: a binary classification\n",
        "\n",
        "you will learn to classify movie reviews as positive or negative ,based on the content of the movie"
      ],
      "metadata": {
        "id": "iTXaLE7uNlGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The IMDB dataset\n",
        " you will work work with the IMDB dataset : a set of 50,000 highly polarized reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing , each set consisting of 50% negative and 50% positive reviews"
      ],
      "metadata": {
        "id": "2j6xPHm6OM3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset"
      ],
      "metadata": {
        "id": "tEyj6A-aO-j4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nYBqqnYg_WID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ceab70-1ece-4b5e-8182-26b4c57cffa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data , train_labels) , (test_data , test_labels) = imdb.load_data(num_words = 10000)\n",
        "# the argument num_words = 100000 means you will only keep the top 10,000 most frequently occuring words in the training data, this allow us to work work with manageble size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variable train__data and test_data are list of reviews ;each review is a list of word indices.train_labels and test_labels are list of 0's and 1's , 0 stand for negatibe and 1 stand for positive"
      ],
      "metadata": {
        "id": "dEu5ci1pRCIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpcj6XUgQ_kP",
        "outputId": "2fabf810-3903-4f0e-b2da-43b3109df223"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Y-3rrfRZtW",
        "outputId": "56f709c4-b5b9-427b-a69f-cc1bc7a8cfaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# as we are restricting ourselves on the top 10,000 nost frequent words , no word index will exceed 10,000\n",
        "max([max(sequence) for sequence in train_data]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srF6gkXrRe8N",
        "outputId": "82490ab5-dece-4c31-ccaf-bac95d9aef52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding reviews back to english word(text)"
      ],
      "metadata": {
        "id": "ws6bhcwCSFIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index() # word_index is a dictionary mapping words to an integer index\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key , value ) in word_index.items()]# reverse it map integer indexes to a word \n",
        ")\n",
        "\n",
        "decode_review = \" \".join(\n",
        "    [reverse_word_index.get(i - 3 , \"?\") for i in train_data[0]]\n",
        ")\n",
        "# decode the review ,Note that the indices are offset by 3 becz 0 and 1 and 2 are reserved indices for \"padding\" , \"start of sequence \" and \"unknown\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yltaSgeuR1fa",
        "outputId": "16ece660-4696-4f64-a048-e329a063bab0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the data\n",
        "\n",
        "you can directly feed the list into the neural network. they all have different length , but neural network expects to process contiguous batches of data, you have to turn your list into tensors\n",
        "\n",
        "1. pad your list so that they all have the same length , turn them into a integer tensor of shaoe(samples, max_length )\n",
        "2. Multi-hot encode your lists to turn them into vector of 0s and 1s"
      ],
      "metadata": {
        "id": "dYyFFRuxWD8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding the integer sequence via multi-hot encoding"
      ],
      "metadata": {
        "id": "A4vQ5SHFXwyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def vectorize_sequences(sequences , dimension = 10000):\n",
        "  results = np.zeros((len(sequences) , dimension)) # create an all -zeros matrix of shape(lrn(sequences), dimension)\n",
        "  for i , sequence in enumerate(sequences):\n",
        "    for j in sequence:\n",
        "      results[i,j] = 1  # sets specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data) # vectorized train data\n",
        "x_test = vectorize_sequences(test_data) # vectorized test data"
      ],
      "metadata": {
        "id": "-OLYZWRqV6pv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejo9JR9gZKwk",
        "outputId": "3241dc18-e8bc-4599-8c39-765574d776ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you should also vectorized you labels\n",
        "y_train = np.array(train_labels).astype(\"float32\")\n",
        "y_test = np.array(test_labels).astype(\"float32\")\n",
        "\n",
        "# now data is ready to fed into neural network"
      ],
      "metadata": {
        "id": "ovijr_aUZTSn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building your model\n",
        "\n",
        "A type of model that performs on such type of problem is a simple stack of densly connected(Dense) layer with relu activation function.\n",
        "There are two key architecture decisions to be made about such a stack of Dense layers.\n",
        "1. how many layers to chose\n",
        "2. How many units to chose for each layer"
      ],
      "metadata": {
        "id": "DfmlL-ayaKho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model definition"
      ],
      "metadata": {
        "id": "GTKYG1WKbAqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation = \"relu\"),\n",
        "    layers.Dense(16, activation = \"relu\"),\n",
        "    layers.Dense(1 , activation = \"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "x-ZWhNgpZnIU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the model"
      ],
      "metadata": {
        "id": "uKBsmf7Ce8jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = \"rmsprop\",\n",
        "    loss = \"binary_crossentropy\",\n",
        "    metrics = [\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "PgjVp9XxbrkB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validating your approach\n",
        "its a standard practice to monitor the accuracy of the model during training ."
      ],
      "metadata": {
        "id": "KHKN-Zh5fXJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting aside a validating set"
      ],
      "metadata": {
        "id": "svq98DZcfokf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "PqzYT3lAfOvr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "OWuLMKRNgID-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "5dvAHAj0f8qd",
        "outputId": "837aa06b-183c-4d61-a35f-fd0382811676"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c14762d40034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpartial_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history = model.fit(partial_x_train,\n\u001b[0m\u001b[1;32m      6\u001b[0m                     \u001b[0mpartial_y_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-1abPqGgtRz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}